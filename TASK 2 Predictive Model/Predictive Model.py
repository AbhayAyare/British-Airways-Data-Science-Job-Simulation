# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jn59tswhiwUk8Mm73Yl1CH08xTtO2aMQ
"""

#imports

import pandas as pd
import numpy as np
import os

import matplotlib.pyplot as plt
import seaborn as sns

#get current working directory

cwd = os.getcwd()

#read the csv

df = pd.read_csv(cwd + "/customer_booking.csv",  encoding="ISO-8859-1")

df.head()

df.shape

df.describe()

df.info()

per_internet = df.sales_channel.value_counts().values[0]  / df.sales_channel.count() *100
per_mobile = df.sales_channel.value_counts().values[1]  / df.sales_channel.count() *100

print(f"Number of bookings done through internet: {per_internet} %")
print(f"Number of bookings done through phone call: {per_mobile} %")

per_round = df.trip_type.value_counts().values[0]/ df.trip_type.count() *100
per_oneway = df.trip_type.value_counts().values[1]/ df.trip_type.count() *100
per_circle = df.trip_type.value_counts().values[2]/ df.trip_type.count() *100

print(f"Percentage of round trips: {per_round} %")
print(f"Percentage of One way trips: {per_oneway} %")
print(f"Percentage of circle trips: {per_circle} %")

plt.figure(figsize=(15,5))
sns.histplot(data=df, x="purchase_lead", binwidth=20,kde=True)

(df.purchase_lead >600).value_counts()

df[df.purchase_lead > 600]

#filtering the data to have only purchase lead days less than 600 days
df = df[df.purchase_lead <600 ]

plt.figure(figsize=(15,5))
sns.histplot(data=df, x="length_of_stay", binwidth=15,kde=True)

(df.length_of_stay> 200).value_counts()

df[df.length_of_stay> 500].booking_complete.value_counts()

#filtering the data to have only length of stay days less than 500 days
df = df[df.purchase_lead <500 ]

mapping = {
    "Mon" : 1,
    "Tue" : 2,
    "Wed" : 3,
    "Thu" : 4,
    "Fri" : 5,
    "Sat" : 6,
    "Sun" : 7
}

df.flight_day = df.flight_day.map(mapping)

df.flight_day.value_counts()

plt.figure(figsize=(15,5))
ax = df.booking_origin.value_counts()[:20].plot(kind="bar")
ax.set_xlabel("Countries")
ax.set_ylabel("Number of bookings")

plt.figure(figsize=(15,5))
ax = df[df.booking_complete ==1].booking_origin.value_counts()[:20].plot(kind="bar")
ax.set_xlabel("Countries")
ax.set_ylabel("Number of complete bookings")

successful_booking_per = df.booking_complete.value_counts().values[0] / len(df) * 100

unsuccessful_booking_per = 100-successful_booking_per

print(f"Out of 50000 booking entries only {round(unsuccessful_booking_per,2)} % bookings were successfull or complete.")

df.to_csv(cwd + "/filtered_customer_booking.csv")

"""**Predictive Model**"""

# imports

import pandas as pd
import numpy as np

import os

import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams.update({'font.size': 14})

# get current working directory
cwd = os.getcwd()

df = pd.read_csv(cwd + "/filtered_customer_booking.csv", index_col=0)

df = df.reset_index(drop=True)
df

df_final = df

from sklearn.preprocessing import OneHotEncoder


#create instance of one hot encoder
encoder = OneHotEncoder(handle_unknown='ignore')

#one hot encode Sales Channel
encoder_df = pd.DataFrame(encoder.fit_transform(df[["sales_channel"]]).toarray())
encoder_df = encoder_df.rename(columns={0:'Internet', 1:'Mobile'})
df_final = df_final.join(encoder_df)

#one hot encode trip type
encoder_df = pd.DataFrame(encoder.fit_transform(df[["trip_type"]]).toarray())
encoder_df = encoder_df.rename(columns={0:'RoundTRip', 1:'OneWayTrip',2:'CircleTrip'})
df_final = df_final.join(encoder_df)

#drop categorical columns now
df_final.drop(['sales_channel', 'trip_type','booking_origin', 'route'], axis=1, inplace = True)

#store the label for supervised learning
label = df['booking_complete']

df_final = df_final.drop('booking_complete', axis=1)

df_final

"""Normalizaing the values"""

from sklearn.preprocessing import StandardScaler

#create a standard scaler object
scaler = StandardScaler()

#fit and transform the data
scaled_df = scaler.fit_transform(df_final)

#create a dataframe of scled data
scaled_df = pd.DataFrame(scaled_df, columns = df_final.columns)

# add the labels back to the dataframe
scaled_df['label']  = label

scaled_df

"""**Correlation matrix**"""

corr = scaled_df.corr()

plt.figure(figsize=(10,7))

#plot the heatmap
sns.heatmap(corr)

import seaborn as sns
import matplotlib.pyplot as plt

# Compute the correlation matrix
corr = scaled_df.corr()

# Set the figure size
plt.figure(figsize=(10, 7))

# Plot the heatmap with a new color map
sns.heatmap(corr, cmap='cividis', linewidths=.5, cbar=True)

# Add title
plt.title('Correlation Heatmap')

from sklearn.model_selection import train_test_split

X = scaled_df.iloc[:,:-1]
y = scaled_df['label']

X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(), y.to_numpy(), test_size=0.20, random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.inspection import permutation_importance

from yellowbrick.classifier import ConfusionMatrix
from sklearn.model_selection import GridSearchCV,  RepeatedStratifiedKFold

"""
    Create functions to fit and predict the values of whether customer would complete the booking.
    Also functions with metrics to evaluate the model prediction.

"""

def model_fit_predict(model, X, y, X_predict):
    model.fit(X, y)
    return model.predict(X_predict)

def acc_score(y_true, y_pred):
    return accuracy_score(y_true, y_pred)

def pre_score(y_true, y_pred):
    return precision_score(y_true, y_pred)

def f_score(y_true, y_pred):
    return f1_score(y_true, y_pred)

#create an instance of the classifier and fit the training data
clf_rf = RandomForestClassifier( max_depth =50 , min_samples_split  =5, random_state=0)

# Identify rows with NaN values in y_train
nan_indices = np.isnan(y_train)

# Drop rows with NaN values from X_train_imputed and y_train
X_train_clean = X_train_imputed[~nan_indices]
y_train_clean = y_train[~nan_indices]

# Now, train your model with X_train_clean and y_train_clean
y_pred_train = model_fit_predict(clf_rf, X_train_clean, y_train_clean, X_train_clean)

# Calculate metrics
f1 = round(f1_score(y_train_clean, y_pred_train), 2)
acc = round(accuracy_score(y_train_clean, y_pred_train), 2)
pre = round(precision_score(y_train_clean, y_pred_train), 2)

print(f"Accuracy, precision, and f1-score for training data are {acc}, {pre}, and {f1} respectively")

cm = ConfusionMatrix(clf_rf, classes=[0, 1], cmap='GnBu')
cm.fit(X_train_clean, y_train_clean)
cm.score(X_train_clean, y_train_clean)

#create an instance of the classifier and fit the training data
clf_rf = RandomForestClassifier(max_depth =50 , min_samples_split=5,random_state=0)

y_pred_test = model_fit_predict(clf_rf, X_train_clean, y_train_clean, X_test)

#f1 score for training data
f1 = round(f1_score(y_test, y_pred_test),2)

#accuracy score for training data
acc = round(accuracy_score(y_test, y_pred_test),2)

#precision score for training data
pre = round(precision_score(y_test, y_pred_test),2)

print(f"Accuracy, precision and f1-score for training data are {acc}, {pre} and {f1} respectively")

cm = ConfusionMatrix(clf_rf, classes=[0, 1], cmap='GnBu')
cm.fit(X_train, y_train)

cm.score(X_test, y_test)

plt.figure(figsize=(10,5))
sorted_idx = clf_rf.feature_importances_.argsort()
plt.barh(scaled_df.iloc[:,:-1].columns[sorted_idx], clf_rf.feature_importances_[sorted_idx])
plt.xlabel("Random Forest Feature Importance")

scaled_df.label.value_counts()

#create a dataframe having all labels 0 with 10000 samples
scaled_df_0 = scaled_df[scaled_df.label ==0].sample(n=8000)

#concatenate the two dataframee, one havng all labels 0 and other having all labels as 1
scaled_df_new = pd.concat([scaled_df[scaled_df.label==1], scaled_df_0], ignore_index=True)

#shuffle the dataframe rows
scaled_df_new = scaled_df_new.sample(frac = 1).reset_index(drop=True)

scaled_df_new

X = scaled_df_new.iloc[:,:-1]
y = scaled_df_new['label']

X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(), y.to_numpy(), test_size=0.20, random_state=42)

#create an instance of the classifier and fit the training data
clf_rf = RandomForestClassifier(n_estimators=50,max_depth =50 , min_samples_split=5,random_state=0)

y_pred_test = model_fit_predict(clf_rf, X_train, y_train, X_test)

#f1 score for training data
f1 = round(f1_score(y_test, y_pred_test),2)

#accuracy score for training data
acc = round(accuracy_score(y_test, y_pred_test),2)

#precision score for training data
pre = round(precision_score(y_test, y_pred_test),2)

recall = round(recall_score(y_test, y_pred_test),2)

specificity = round(recall_score(y_test, y_pred_test, pos_label=0),2)
print(acc)
print(pre)
print(recall)
print(f"Accuracy, precision, recall and f1-score for training data are {acc}, {pre}, {recall}, {specificity} and {f1} respectively")

cm = ConfusionMatrix(clf_rf, classes=[0, 1], cmap='GnBu')
cm.fit(X_train, y_train)

cm.score(X_test, y_test)

# plt.figure(figsize=(10,8))
# sorted_idx = clf_rf.feature_importances_.argsort()
# plt.barh(scaled_df.iloc[:,:-1].columns[sorted_idx], clf_rf.feature_importances_[sorted_idx])
# plt.xlabel("Random Forest Feature Importance")
plt.figure(figsize=(10, 8))
sorted_idx = clf_rf.feature_importances_.argsort()

# Generate a list of colors using a colormap
colors = plt.cm.viridis(np.linspace(0, 1, len(sorted_idx)))

plt.barh(scaled_df.iloc[:, :-1].columns[sorted_idx], clf_rf.feature_importances_[sorted_idx], color=colors)
plt.xlabel("Random Forest Feature Importance")
plt.title("Feature Importance with Different Colors")
plt.show()

